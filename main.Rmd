---
title: "Proyecto Grupal - Análisis de Datos I"
author: 
  - Luis Fernando Amey Apuy - C20470
  - Javier Hernández Navarro - C13674
  - Gustavo Alberto Amador Fonseca - C20459
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
    self_contained: true
    highlight: tango
---

# Librerías y carga de datos
```{r}
source("cod/r/setup.R")
```

```{r}
gen <- scrapW("genfiltrada")
# herra <- scrapT("herraII_08-07-24")
```

# Gráficos
```{r, warning=FALSE}
cuentas <- gen %>%
  mutate(mes = floor_date(as.Date(dia), "month")) %>%
  count(mes)

# Gráfico con la cantidad de mensajes por mes y año
ggplot(cuentas, aes(x = mes, y = n)) +
  geom_line(color = "salmon", size = 1) +
  geom_point(color = "salmon", size = 2) + 
  labs(x = "Mes-Año",
       y = "Cantidad de mensajes") +
  theme_minimal()

```

```{r}
# Gráfico con las horas registradas por día
ggplot(gen, aes(x = dia, y = hora)) +
  geom_point(color = "darkblue", alpha = 0.6) +
  labs(x = "Día",
       y = "Hora (seg)") +
  theme_minimal()

```

```{r}
cuentas <- gen%>%count(autor) %>% arrange(desc(n))
cuentas$autor <- factor(cuentas$autor, levels = cuentas$autor)
cuentas <- cuentas[cuentas$n >70,]

# Gráfico con el top de autores con más mensajes
ggplot(cuentas, aes(x = autor, y = n)) +
  geom_col(fill = "darkorange", color="black") +
  labs(x = "Autores",
       y = "Cantidad de mensajes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 0.8))
```

```{r, warning =FALSE}
cuentas <- gen%>%count(hour(hora)) %>% arrange(desc(n))

# Gráfico con la cantidad de mensajes por hora
ggplot(cuentas, aes(x = `hour(hora)`, y = n)) +
  geom_col(fill = "steelblue", color="darkblue") +
  labs(x = "Horas",
       y = "Cantidad de mensajes") +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0.8))
```

```{r}
copia <- gen
rangos <- gen %>%
  group_by(autor) %>%
  summarise(rango = max(hora) - min(hora)) %>%
  arrange(rango)

# Reordenar el factor 'autor' según el rango
copia$autor <- factor(gen$autor, levels = rangos$autor)

# Graficar boxplot
ggplot(copia, aes(y = autor, x = hora / 3600)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  labs(x = "Hora",
       y = "Autor") +
  theme_minimal()
```

```{r}
cuentas <- gen %>% group_by(autor) %>% 
  summarise(m_editados = sum(editado)) %>% 
  arrange(desc(m_editados))
cuentas <- cuentas[cuentas$m_editados >0,]
cuentas$autor <- factor(cuentas$autor, levels = cuentas$autor)
ggplot(cuentas, aes(x = autor, y = m_editados)) +
  geom_col(fill = "darkorange", color="black") +
  labs(title = "Mayor cantidad de mensajes editados",
       x = "Autores",
       y = "Número de mensajes editados") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 0.8))

```

# Mapa de palabras
```{r}
palabras <- gen$mensaje
mensajes_clean <- gsub("[[:punct:]]", "", tolower(palabras))
totales <- data.frame(palabras = unlist(strsplit(mensajes_clean, "\\s+")))
cuentas <- totales %>% count(palabras)
```

```{r}
wordcloud(words = cuentas$palabras,
          freq = cuentas$n,
          min.freq = 8,
          colors = colorRampPalette(brewer.pal(8, "Dark2"))(50),
          random.order = FALSE)
```

# Preparación para clusterización
Se hará un caso puro sin eliminación y el otro caso con selección
## Selección de mensajes significativos
Los mensajes significativos serán todos aquellos que no tengan stickers, 
imágenes, documentos etc. Todos estos mensajes deberían ser completamente
inútiles para un análisis, puesto que solo contienen eso.

```{r}
gen_c <- gen
gen_c$id <- 1:3675
copia <- gen_c %>%
  filter(!grepl("sticker omitido|imagen omitida|Se eliminó este mensaje|documento omitido|Cambió tu código de seguridad|Video omitido|añadió|https|usando el enlace de invitación|eliminó|cambió su número de teléfono", mensaje))
```
Se deciden eliminar los links puesto que después de una revisión manual son muy 
pocos los casos significativos 

## Eliminación de encuestas 
```{r}
copia <- copia %>% 
  filter(!grepl("ENCUESTA:|OPCIÓN:", mensaje))
```

## Eliminación del autor de control de Whatsapp
```{r}
copia <- copia[copia$autor != 'Autor 0',]
```

## Tokenización de las palabras
```{r}
tokens <- copia %>%
  unnest_tokens(palabras, mensaje)
```

## Eliminación de las menciones (@numtelefono) y enteros
```{r}
tokens <- tokens %>%
  filter(!grepl("^\\d+$", palabras))
```

## Eliminación de palabras monótonas
```{r}
palabras <- copia$mensaje
mensajes_clean <- gsub("[[:punct:]]", "", tolower(palabras))
totales <- data.frame(palabras = unlist(strsplit(mensajes_clean, "\\s+")))
cuentas <- totales %>% count(palabras)
```

### Mapeo de palabras con eliminaciones
```{r, warning=FALSE}
wordcloud(words = cuentas$palabras,
          freq = cuentas$n,
          min.freq = 10,
          colors = colorRampPalette(brewer.pal(8, "Dark2"))(50),
          random.order = FALSE)
```

### Eliminación de palabras más repetidas sin sentimiento (43 veces o más)
```{r}
cuentas <- cuentas[cuentas$n >= 43,]
cuentas <- cuentas %>% mutate(get_nrc_sentiment(palabras, lang="spanish"))
```

```{r, warning =FALSE}
cuentas %>%
  summarise(across(3:ncol(.), sum, na.rm = TRUE))
```
Al no haber sentimientos, se pueden optar por remover en este caso. 

```{r}
tokens <- tokens %>%
  anti_join(cuentas, by = "palabras")
```

### Visualización después de eliminación
```{r}
palabras <- tokens$palabras
mensajes_clean <- gsub("[[:punct:]]", "", tolower(palabras))
totales <- data.frame(palabras = unlist(strsplit(mensajes_clean, "\\s+")))
cuentas <- totales %>% count(palabras)
```

```{r, warning=FALSE}
pdf("res/wordcloud.pdf", width = 8, height = 6)

wordcloud(words = cuentas$palabras,
          freq = cuentas$n,
          min.freq = 8,
          scale = c(1.8, 0.2),
          colors = colorRampPalette(brewer.pal(8, "Dark2"))(50),
          random.order = FALSE)

dev.off()

```

# Modelos
## Modelo original (sin eliminación)
```{r}
mdl_o <- gen_c %>%
  unnest_tokens(palabras, mensaje)
```

## Modelo filtrado (con eliminación)
Se puede notar una reducción de la mitad de las observaciones
```{r}
mdl_s <- tokens
```

### Acomodo del ambiente
```{r}
rm(copia, 
   cuentas,
   rangos, 
   tokens, 
   totales,
   palabras,
   mensajes_clean)
```

## Sentimientos de los modelos
```{r}
t <- proc.time()
mdl_o <- mdl_o %>% mutate(get_nrc_sentiment(palabras, lang="spanish"))
mdl_s <- mdl_s %>% mutate(get_nrc_sentiment(palabras, lang="spanish"))
# proc.time() - t
```

## Sentimientos generales
```{r}
mdl_o %>%
  summarise(across(7:ncol(.), sum, na.rm = TRUE))
```

```{r}
mdl_s %>%
  summarise(across(7:ncol(.), sum, na.rm = TRUE))
```

## Agrupación por mensajes de nuevo
```{r}
mdl_o <- mdl_o %>%
  group_by(id) %>%
  summarise(across(anger:positive, sum), .groups = "drop")

mdl_s <- mdl_s %>%
  group_by(id) %>%
  summarise(across(anger:positive, sum), .groups = "drop")
```

```{r}
mdl_o <- inner_join(gen_c, mdl_o, by = join_by(id))
mdl_s <- inner_join(gen_c, mdl_s, by = join_by(id))
```

Hay menos mensajes en el original puesto que los emojis solos no se tokenizan.

```{r}
rm(gen_c)
```

## Eliminación de las variables no significativas (sin sentimientos)
Se procederá a dividir los modelos más, un lado sin los sentimientos en 0

```{r}
ind_o <- rowSums(mdl_o[,7:15] != 0) != 0
ind_s <- rowSums(mdl_s[,7:15] != 0) != 0
```

```{r}
mdl_1 <- mdl_o
mdl_2 <- mdl_s
mdl_3 <- mdl_o[ind_o,]
mdl_4 <- mdl_s[ind_s,]
```

```{r}
rm(ind_o,
   ind_s,
   mdl_o,
   mdl_s)
```

# Clusterización por medio de métodos vistos en clase 
## Funciones de clusterización
```{r}
colocar <- function(ind, centroids, past){
  dist <- sapply(1:nrow(centroids), function(x) sum((centroids[x,] - ind)^2))
  posibles <- which(dist == min(dist))
  if(length(posibles) == 1){
    return(posibles[1])
  } else if(sum(posibles == past) > 0){
    return(past)
  } else {
    return(posibles[1])
  }
}
```

### Kmeans
```{r}
kmeans <- function(nodes, df){
  len <- nrow(df)
  ids <- sample(len, nodes)
  val <- df[,7:16]
  centr <- val[ids,]
  clusters <- rep(0, len)
  clusters[ids] <- 1:nodes
  past <- clusters
  numiter <- 0
  while(numiter < 100){
    clusters <- sapply(1:len, function(x) colocar(val[x,],centr, past[x]))
    print(abs(sum(clusters!=past)))
    if(abs(sum(clusters!=past)) == 0){
      break
    } else{
      past <- clusters
    }
    numiter <- numiter + 1
    centr <- t(sapply(1:nodes, function(x) 
      sapply(1:9, function (y) mean(val[clusters == x, y]))
    ))
  }
  return(list(clusters, centr))
}
```

### Kmedians  
```{r}
kmedians <- function(nodes, df){
  len <- nrow(df)
  ids <- sample(len, nodes)
  val <- df[,7:16]
  centr <- val[ids,]
  clusters <- rep(0, len)
  clusters[ids] <- 1:nodes
  past <- clusters
  numiter <- 0
  while(numiter < 100){
    clusters <- sapply(1:len, function(x) colocar(val[x,],centr, past[x]))
    print(abs(sum(clusters!=past)))
    if(abs(sum(clusters!=past)) == 0){
      break
    } else{
      past <- clusters
    }
    numiter <- numiter + 1
    new_centr <- t(sapply(1:nodes, function(x) 
      sapply(1:9, function (y) median(val[clusters == x, y]))
    ))
    new_centr <- sapply(1:nodes, function(x) colocar(new_centr[x,], val[clusters == x, ], 0))
    centr <- t(sapply(1:nodes, function(x) val[clusters == x, ][new_centr[x],]))
    ids <- sapply(1:nodes, function(x) df[clusters == x, 6][new_centr[x]])
  }
  return(list(clusters, centr, ids))
}
```

### Prueba de kmeans
```{r}
t <- proc.time()
clusters <- 5
clus_mean <- kmeans(clusters, mdl_4)
proc.time() - t
```

### Resumen clusters
```{r}
sapply(1:clusters, function(x) colSums(mdl_4[clus_mean[[1]] == x,7:16]))
```

```{r}
sapply(1:clusters, function(x) sum(clus_mean[[1]] == x))
```

### Prueba de kmedians
```{r}
t <- proc.time()
clus_median <- kmedians(5, mdl_4)
proc.time() - t
```

De este se pueden sacar las observaciones que son centroides para su interpretación
```{r}
sapply(1:clusters, function(x) mdl_4[mdl_4$id == clus_median[[3]][x], 4])
```

### Paralelización de las funciones
Sirve para hacer simulaciones y encontrar el set de clusters más usado
```{r}
kmeans_par <- function(n, clst, mdl, cores) {
  cl <- makeCluster(cores) 
  clusterExport(
    cl,
    varlist = c("clusters","mdl_1","mdl_2","mdl_3","mdl_4","kmeans","colocar"),
    envir = environment())
  resultados <- parSapply(cl, 1:n, function(x) kmeans(clst, mdl)[[2]])
  stopCluster(cl)
  return(resultados)
}
```

```{r}
kmedians_par <- function(n, clst, mdl, cores) {
  cl <- makeCluster(cores) 
  clusterExport(
    cl,
    varlist = c("clusters","mdl_1","mdl_2","mdl_3","mdl_4","kmedians","colocar"),
    envir = environment())
  resultados <- parSapply(cl, 1:n, function(x) kmedians(clst, mdl)[[3]])
  stopCluster(cl)
  return(resultados)
}
```

### Prueba algoritmo 
```{r}
# t <- proc.time()
# clusters <- 5
# res5 <- kmedians_par(1000, clusters, mdl_4, 6)
# proc.time() - t
# write.csv(res5, "res/res5.csv")
res5 <- read.csv("res/res5.csv") %>% select(-1)
```

### Obtener el set de centroides moda para kmedians
```{r}
centr <- t(sapply(data.frame(res5), function(x) sort(x)))
df <- apply(centr, 1, paste, collapse = "-")
tabla <- sort(table(df), decreasing = TRUE)
tabla[tabla>5]
```

### Obtener los centroides moda para kmedians
```{r}
tabla <- sort(table(as.vector(res5)), decreasing = T)
tabla[tabla>100]
```

### Recuperación de los clusters
```{r}
indices <- c(2,41,98,142,373)
centr <- t(sapply(1:5, function(x) mdl_4[mdl_4$id == indices[x], 7:16]))
clusters <- sapply(1:640, function(x) colocar(mdl_4[x,7:16], centr, 0))
```

```{r}
sapply(1:5, function(x) colSums(mdl_4[clusters == x,7:16]))
```

```{r}
sapply(1:5, function(x) sum(clusters == x))
```

## Obtención de la clusterización óptima por medio del método del codo
```{r}
wcss <- function(nodes, df){
  res <- kmeans(i, df)
  points_c <- sapply(1:nodes, function(x) sum(res[[1]] == x))
  wcss <- sum(sapply(1:nodes, function(x) sum(sapply(
    1:points_c[x], function(y) sum((df[res[[1]] == x, 7:16][y,] - res[[2]][x,])^2)
  ))))
  return(wcss)
}
```

```{r}
t <- proc.time()
metrica <- data.frame(mdl1 = rep(0,10),
                      mdl2 = rep(0,10),
                      mdl3 = rep(0,10),
                      mdl4 = rep(0,10))
for(i in 1:10){
  print(i)
  metrica$mdl1[i] <- wcss(i, mdl_1)
  metrica$mdl2[i] <- wcss(i, mdl_2)
  metrica$mdl3[i] <- wcss(i, mdl_3)
  metrica$mdl4[i] <- wcss(i, mdl_4)
}
proc.time() - t
metrica$x <- 1:10
metrica <- metrica %>%
  pivot_longer(cols = starts_with("mdl"), 
               names_to = "modelo", 
               values_to = "valor")
metrica_median <- metrica
write.csv(metrica_median, "res/metrica_median.csv")
```

```{r}
fig <- ggplot(metrica_mean, aes(x = x, y = valor, color = modelo)) +
  geom_line() +
  geom_point(show.legend = FALSE) +  
  scale_color_manual(values = c("mdl1" = "darkorange", 
                                "mdl2" = "cyan", 
                                "mdl3" = "red", 
                                "mdl4" = "blue"), 
                     labels = c("mdl1" = "Modelo 1", 
                                "mdl2" = "Modelo 2", 
                                "mdl3" = "Modelo 3", 
                                "mdl4" = "Modelo 4")) +
  theme_minimal() +
  labs(x = "clusters",
       y = "WCSS",
       color = "Modelos") +
  theme(legend.position = "right")
ggsave("res/kmeans.pdf", fig, width = 6, height = 4)

```

```{r}
fig <- ggplot(metrica_median, aes(x = x, y = valor, color = modelo)) +
  geom_line() +
  geom_point(show.legend = FALSE) +  
  scale_color_manual(values = c("mdl1" = "darkorange", 
                                "mdl2" = "cyan", 
                                "mdl3" = "red", 
                                "mdl4" = "blue"), 
                     labels = c("mdl1" = "Modelo 1", 
                                "mdl2" = "Modelo 2", 
                                "mdl3" = "Modelo 3", 
                                "mdl4" = "Modelo 4")) +
  theme_minimal() +
  labs(x = "clusters",
       y = "WCSS",
       color = "Modelos") +
  theme(legend.position = "right")

ggsave("res/kmedians.pdf", fig, width = 6, height = 4)
```

## Obtención del mejor modelo
Puesto lo anterior, se quedará solo con el modelo 4, al presentar mejores agrupaciones
```{r}
modelo <- mdl_4[,7:16]
cat_mdl <- mdl_4
```

```{r}
rm(mdl_1,
   mdl_2,
   mdl_3, 
   mdl_4,
   kmeans,
   kmedians, 
   colocar, 
   t)
```

## Acomodo de clusters
```{r}
clusters <- data.frame(kmeans = clus_mean[[1]],
                       kmedians = clus_median[[1]],
                       jerarquico = 0,
                       dbscan = 0,
                       gmm = 0,
                       som = 0,
                       spectral = 0,
                       teigen = 0,
                       meanshift = 0)
```

```{r}
rm(clus_mean,
   clus_median)
```


## Clustering Jerárquico
```{r}
clusters$jerarquico <- cutree(hclust(dist(modelo), method = "ward.D2"), k = 5)
table(clusters$jerarquico)
```

# Clusterización por medio de métodos externos a las clases
## DBSCAN
Hay que tomar un intervalo entero por discretización
```{r}
clusters$dbscan <- dbscan(modelo, eps = 1, minPts = 20)$cluster + 1
table(clusters$dbscan)
```

## GMM
```{r}
clusters$gmm <- Mclust(modelo)$classification
table(clusters$gmm)
```

## SOM
```{r}
clusters$som <- som(scale(modelo), somgrid(xdim = 3, ydim = 3, topo = "hexagonal"), rlen = 100)$unit.classif
table(clusters$som)
```

## Spectral Clustering
```{r}
clusters$spectral <- specc(as.matrix(modelo), centers = 5)
table(clusters$spectral)
```

## Modelos de mezcla no Gaussianos
```{r}
# 3 clusters es el máximo que converge
clusters$teigen <- teigen(modelo, Gs = 3)$classification
table(clusters$teigen )
```

## Mean Shift Clustering
```{r}
clusters$meanshift <- meanShift(scale(modelo), epsilonCluster = 2.5)$assignment[,1]
table(clusters$meanshift)
```

# Comparación de modelos 

## Coeficiente de Silhouette
Se sitúa entre -1 y 1. Más alto mejor. Parece un índice de parsimonía
```{r}
silhouette <- sapply(1:9, function(x) mean(silhouette(clusters[,x], dist(modelo))[, 3]))
```

## Índice de Calinski-Harabasz
Más alto mejor
```{r}
cal_har <- sapply(1:9, function(x) intCriteria(as.matrix(modelo), as.integer(clusters[,x]), "Calinski_Harabasz")$calinski_harabasz)
```

## Índice de Davies-Bouldin
Más bajo es mejor
```{r}
dav_bou <- sapply(1:9, function(x) intCriteria(as.matrix(modelo), as.integer(clusters[,x]), "Davies_Bouldin")$davies_bouldin)
```


## Resumen estadísticas
```{r}
nclus <- sapply(1:9, function(x) max(clusters[,x]))
resumen <- data.frame(método = names(clusters),
                      clusters = nclus,
                      silhouette,
                      cal_har,
                      dav_bou)
resumen 
```

## Elegir el método más adecuado
Se descartan por ser los peores:
- gmm: por tener el tercero más alto
- dbscan: por tener el segundo más bajo
- kmedians: por tener el primero más bajo

```{r}
(resumen %>% slice(-c(2,4,5)))
```

De nuevo descartamos por ser los peores:
- kmeans en el primero y tercero
- meanshift en el segundo

Note que el jerárquico y meanshift ajustan de primeros en otras métricas, pero nos interesa una mejor elección en todas

```{r}
(resumen %>% slice(-c(1,2,4,5,9)))
```

Por último, se descarta
- jerarquico por ser el peor en el primero
- spectral por ser el peor en el tercero 
- teigen por ser el peor en el segundo

# Modelo escogido
Por lo tanto, el modelo som, o Self Organizing Maps es el escogido
## Estadísticas breves
```{r}
set.seed(16)
som <- som(scale(modelo), somgrid(xdim = 3, ydim = 3, topo = "hexagonal"), rlen = 100)
(c_emotions <- sapply(1:9, function(x) colSums(cat_mdl[som$unit.classif == x,7:16])))
```

Defina los clusters:
- 1. El cluster de confianza (155 y 108 positivo)
- 2. El cluster de felicidad (50% de los datos)
- 3. El cluster de espontaneidad (46% sorpresa, 40% felicidad y 30% anticipación)
- 4. El cluster de la anticipación (30% anticipación sola)
- 5. El cluster de la frustración (27% enojo, 21% miedo)
- 6. El cluster de los mensajes largos (2 observaciones con mucho peso)
- 7. El cluster de la tristeza (46% sadness)
- 8. El cluster del miedo (40% sorpresa y 21% miedo)
- 9. El cluster pesimista (40% anger, 70% disgust, 30% negative)

```{r}
round(c_emotions/rowSums(c_emotions),2)
```

```{r}
sapply(1:9, function(x) sum(som$unit.classif == x))
```

```{r}
pdf("res/som_graficos.pdf", width = 10, height = 5)  # Puedes ajustar tamaño

par(mfrow = c(1, 2))
plot(som)
plot(som, type = "mapping", main = "SOM Clustering")
par(mfrow = c(1, 1))

dev.off()
```

## Mensajes centroides
```{r}
centr <- sapply(1:9, function(x) sapply(1:10, function(y) median(modelo[som$unit.classif == x,y])))
```

```{r}
dist <- sapply(1:9, function(x) 
  sapply(1:length(modelo[som$unit.classif == x,1]), 
         function(y) sum((modelo[som$unit.classif == x,][y,] - centr[,x])^2)))
set.seed(15)
tomar <- sapply(1:9, function(x) sample(which(dist[[x]] == min(dist[[x]])),1))
```

```{r}
sapply(1:9, function(x) cat_mdl$mensaje[som$unit.classif == x][tomar[x]])
```

```{r}
df <- data.frame(
  autor   = cat_mdl$autor,
  cluster = som$unit.classif
)

df_prop <- df %>%
  count(autor, cluster, name = "n") %>%
  group_by(autor) %>%
  mutate(
    total_autor = sum(n),
    prop        = n / total_autor
  ) %>%
  ungroup()

df_heat <- df_prop %>%
  select(autor, cluster, prop) %>%
  pivot_wider(names_from = cluster, values_from = prop, values_fill = 0)

fig <- ggplot(df_prop, aes(x = factor(cluster), y = autor, fill = prop)) +
  geom_tile(color = "black") +
  scale_fill_gradient(
    name = "Proporción",
    low = "red",   # color para valores bajos
    high = "green" # color para valores altos
  ) +
  labs(
    x = "Cluster SOM",
    y = "Autor",
    title = "Proporción de mensajes de cada autor por cluster"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))
ggsave("res/heatmap.pdf", fig)
```

```{r}
df_dom <- df_prop %>%
  group_by(autor) %>%
  slice_max(prop, n = 1, with_ties = FALSE) %>%
  select(autor, cluster, prop) %>%
  arrange(desc(prop))

fig <- ggplot(df_dom, aes(x = reorder(autor, prop), y = prop, fill = factor(cluster))) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    x = "Autor",
    y = "Proporción en su cluster dominante",
    fill = "Cluster",
    title = "Cluster dominante de cada autor"
  ) +
  theme_minimal()
ggsave("res/dominante.pdf", fig)
```























